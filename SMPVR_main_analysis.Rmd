---
title: <font size="5">**SMPVR -- Main Analysis**</font> 
author: <br> <font size="4"> Pawe³ Motyka (University of Warsaw) </font> <br>  *pawel.motyka@psych.uw.edu.pl* 
date: <font size="3"> October 2019  </font>
output: html_document
chunk_output_type: console

--- 

&nbsp;

<font size="4">
**List of sections**:

1. Load the required packages and the preprocessed data [S1](#S1)
2. Derive alteration rates [S2](#S2)
3. Perform stepwise exclusions [S3](#S3)
4. Test differences in alterations and mixed percepts between walking conditions [S4](#S4)
5. Test differences in perceived optic flows between walking conditions [S5](#S5)
6. Plot differences in perceived optic flows between walking conditions  [S6](#S6)
7. Test and plot differences in perceived optic flows between walking conditions (unfiltered sample) [S7](#S7)
8. Relation between proprioceptive precision and the predominance of congruent optic flows [S8](#S8)
9. Self-reported levels of simulator sickness and sense of presence [S9](#S9)


<a name="S1"></a>
&nbsp;

#####**1. Load the required packages and the preprocessed data** 


```{r, results = "hide", message = FALSE, warning = FALSE}

require(dplyr, warn.conflicts = FALSE, quietly=TRUE)
require(psych, warn.conflicts = FALSE, quietly=TRUE)
require(lm.beta, warn.conflicts = FALSE, quietly=TRUE)
require(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
require(scales, warn.conflicts = FALSE, quietly=TRUE)
require(lme4, warn.conflicts = FALSE, quietly=TRUE)
require(tidyr, warn.conflicts = FALSE, quietly=TRUE)
require(afex, warn.conflicts = FALSE, quietly=TRUE)
require(plyr, warn.conflicts = FALSE, quietly=TRUE)
require(emmeans, warn.conflicts = FALSE, quietly=TRUE)
require(MBESS, warn.conflicts = FALSE, quietly=TRUE)
require(colorspace, warn.conflicts = FALSE, quietly=TRUE)
require(DescTools, warn.conflicts = FALSE, quietly=TRUE)
require(corrplot, warn.conflicts = FALSE, quietly=TRUE)
require(effsize, warn.conflicts = FALSE, quietly=TRUE)
require(here, warn.conflicts = FALSE, quietly=TRUE)

data_dir <- paste0(here(),"/_data")
setwd(data_dir)

options(width=100)

### Proprioception Assessment
data_proprioception <- read.table("SMPVR_proprioception_processed_data", header = TRUE, sep = ",", fill = TRUE, stringsAsFactors = FALSE)
## Crucial variables:
# diff_M - mean reproduction error
# diff_M_abs - mean absolute reproduction error
# diff_SD - variance of reprodcution errors
# other notations: "Ab"- abductions, "Fl" - flexions, "60"/"90"/"120" - target positions in degrees

### Questionnaire measures
data_questionnaires <-  read.table("SMPVR_questionnaire_data", header = T,  sep = "\t", fill = TRUE, stringsAsFactors = FALSE)
## Crucial variables:
# Pre1-7 - Simulator Sickness Questionnaire items (pre)
# Post1-7 - Simulator Sickness Questionnaire items (post)
# VR1-6 - Slater-Usoh-Steed Questionnaire (presence in virtual environment)
# Resp_Confidence - reported confidence of button pressess during binocular rivalry (Likert scale 1-7)

### Main data: durations of individual rensponses during binocular rivalry
data_d <- read.table("SMPVR_main_data_processed", header = TRUE, sep = "\t", fill = TRUE, stringsAsFactors = FALSE)
# rename variables
data_d <- plyr::rename(data_d, replace = c("WalkDirectionForwardLog" = "walking"))
data_d$walking <- as.character(data_d$walking)
data_d$walking <- plyr::revalue(data_d$walking, c("0" = "backward", "1" = "forward"))
data_d <- plyr::rename(data_d, replace = c("time" = "duration"))
## Crucial variables:
# walking - forward/backward
# forwardEyeLog - Left/Right (which eye is presented with forward optic flow)
# forwardEyeLogColor - Green/Red (in which color forward optic flow is presented)
# leftButtonLog- 1/0 (is left part of touchpad pressed)
# rightButtonLog- 1/0 (is right part of touchpad pressed)
# p_color - Green/Red/non (currently perceived color)
# p_opticflow - forward/backward/non (currently perceived optic flow direction)
# duration - duration of a given response in seconds

```

<a name="S2"></a>
&nbsp;

#####**2. Derive alteration rates** 

```{r}

## derive full alterations - defined as perceptual switches from one image to another (e.g., left–none–right button presses), excluding return transitions (e.g., left–none–left). 

# prepare dataframe
data_alt_blocks <- data.frame(ID = integer(0),
                      block = integer(0),
                      full_alterations = integer(0))

# save data as separate object
data_alt <- data_d

# remove durations with no response (mixed percepts) 
data_alt <- data_alt[data_alt$p_opticflow != "non",]

# get the list of subjects
sub_list <- unique(data_d$ID)
for (s in sub_list) { # start loop: participants
  
block_list <- unique(data_alt$blockLog[data_alt$ID == s])
for (b in block_list) {# start loop: blocks
 
df <- data_alt[data_alt$ID == s & data_alt$blockLog == b,]

# check for switches in sequence of consecutive responses
df = df %>%
  mutate(groupChanged = (p_opticflow != lag(p_opticflow, default = p_opticflow[1]))
         , toCutBy = cumsum(groupChanged)) %>%
  group_by(toCutBy) %>%
slice(c(1, n()))

# get number of full alterations for a given block
full_alterations <- max(df$toCutBy)

# save number of full alterations to dataframe
data_alt_blocks[nrow(data_alt_blocks)+1,] <- c(s, b, full_alterations)  
} # end loop: blocks
} # end loop: participants

# save as numeric variables
data_alt_blocks$full_alterations <- as.numeric(as.character(data_alt_blocks$full_alterations))

# summarize alterations per subject
data_alt_ID <- data_alt_blocks %>%
  group_by(ID) %>%
  dplyr::summarize(alterations = mean(full_alterations))

# Standardize mean alteration rate
data_alt_ID$z_alterations <- (data_alt_ID$alterations - mean(data_alt_ID$alterations))/sd(data_alt_ID$alterations)

# mean and SD for number of alterations per block
mean(data_alt_ID$alterations)
sd(data_alt_ID$alterations)

# plot distribution of alterations per block
#hist(data_alt_ID$alterations, breaks = 15, xlab = "Alerations per block", main = " ", col = "grey60", cex.lab = 1.4, cex.axis = 1.4)

# save as separate dataframe
dt <- data_alt_ID

```

<a name="S3"></a>
&nbsp;

#####**3. Perform stepwise exclusions ** 

```{r}

## (1) Criterion of minimal alteration rate 

# check outliers with respect to z-values (according to a cut-off criterion of three standard deviations below or above sample mean))
data_alt_ID$ID[data_alt_ID$z_alterations > 3 | data_alt_ID$z_alterations < -3]

# Even though there were no 'formal' outliers one person with virtually no alterations (< 1) has been excluded (arbitrary criterion of 4 full alterations per block))
length(unique(data_alt_ID$ID[data_alt_ID$alterations < 4]))

# The next minimal (nonexcluded) value of alterations equals:
min(data_alt_ID$alterations[data_alt_ID$alterations > 4])

# Specify which subjects are to be removed due to this criterion
exclusion_list_alterations <- unique(data_alt_ID$ID[data_alt_ID$alterations < 4])


## (2) Criterion of minimal average duration of percepts (forward & backward taken together) 

# removed no responses (mixed percepts)
data_d_filt <- data_d[data_d$p_opticflow != "non",]

# create dataframe with average duration of precepts
dur_percepts <- data_d_filt %>%
  group_by(ID) %>%
  dplyr:::summarize(dur_perc = mean(duration))

# standardize duration of percepts
dur_percepts$z_dur_perc <- (dur_percepts$dur_perc - mean(dur_percepts$dur_perc))/sd(dur_percepts$dur_perc)

# look for outliers (according to a cut-off criterion of three standard deviations below or above sample mean)
dur_percepts$ID[dur_percepts$z_dur_perc > 3 | dur_percepts$z_dur_perc < -3]

# Specify which subjects are to be removed due to this criterion
exclusion_list_percept_duration <- dur_percepts$ID[dur_percepts$z_dur_perc > 3 | dur_percepts$z_dur_perc < -3]

# what is z-value for this subject?
dur_percepts$z_dur_perc[dur_percepts$ID == "6"]

# explore other outliers through log transformation
#dur_percepts$dur_perc_log <- log(dur_percepts$dur_perc)
#hist(dur_percepts$dur_perc_log)

# additional exploration indicated cluster of subjects had very short mean percepts (< 1/3 s) with no outlying z-values
length(unique(dur_percepts$ID[dur_percepts$dur_perc < 0.75]))
additional_outliers <- unique(dur_percepts$ID[dur_percepts$dur_perc < 0.75])

# Specify which subjects are to be removed due to this criterion
exclusion_list_percept_duration  <- append(exclusion_list_percept_duration, additional_outliers)
      
# the next minimal (nonexcluded) value
min(dur_percepts$dur_perc[dur_percepts$dur_perc > 0.333])

# merge with alterations dataframe
dt <- merge(dt, dur_percepts, by = "ID")


## (3) Criterion of minimal average duration of mixed percepts

# derive mean durations of mixed percepts (in seconds)
dur_non <- data_d %>%
  group_by(ID, p_opticflow) %>%
  dplyr:::summarize(dur_non = sum(duration))

# keep only mixed percepts in the dataframe
dur_non <- dur_non[dur_non$p_opticflow == "non",]

# standardize duration of mixed percepts
dur_non$z_dur_non <- (dur_non$dur_non - mean(dur_non$dur_non))/sd(dur_non$dur_non)

# look for outliers (according to a cut-off criterion of three standard deviations below or above sample mean)
dur_non$ID[dur_non$z_dur_non > 3 | dur_non$z_dur_non < -3]

df_total_time <- data_d %>%
  group_by(ID) %>%
  dplyr:::summarize(total_time = sum(duration)) 
# note that total time do not sum exactly to 8*120s since small time differences between consecutive button presses (rows) were not assigned to any of response durations (these gaps equal ~0.025s given 40 Hz sampling rate).

# calculate mixed percepts rate in % (duration (in s)/total time (in s))
dur_non <- merge(dur_non, df_total_time, by = c("ID"))
dur_non$duration <- dur_non$dur_non / dur_non$total_time

# the cluster of subjects with highest rate of mixed percpets overlap fully with exclusions based on the Criterion 2.

# Specify which subjects are to be removed due to this criterion 
exclusion_list_mixed_percepts <- unique(dur_non$ID[dur_non$z_dur_non > 3 | dur_non$z_dur_non < -3])
exclusion_list_mixed_percepts

# merge with other behavioral dataframes
dt <- merge(dt, dur_non, by = "ID")

# save original unfiltered data (before exclusions)
data_d_unfilt <- data_d

## Perform Exclusions (main dataframe)

data_d <- data_d[!(data_d$ID %in% exclusion_list_alterations), ]
length(unique(data_d$ID))

data_d <- data_d[!(data_d$ID %in% exclusion_list_percept_duration), ]
length(unique(data_d$ID))

data_d <- data_d[!(data_d$ID %in% exclusion_list_mixed_percepts), ]
length(unique(data_d$ID))

## Perform Exclusions (alterations dataframe) ###

length(unique(data_alt_blocks$ID))

data_alt_blocks <- data_alt_blocks[!(data_alt_blocks$ID %in% exclusion_list_alterations), ]
length(unique(data_alt_blocks$ID))

data_alt_blocks <- data_alt_blocks[!(data_alt_blocks$ID %in% exclusion_list_percept_duration), ]
length(unique(data_alt_blocks$ID))

data_alt_blocks <- data_alt_blocks[!(data_alt_blocks$ID %in% exclusion_list_mixed_percepts), ]
length(unique(data_alt_blocks$ID))


```

<a name="S4"></a>
&nbsp;

#####**4. Test differences in alterations and mixed percepts between walking conditions ** 

```{r}

### Alterations

# recode blocks according to walking direction
data_alt_blocks$direction[data_alt_blocks$block == 1 | data_alt_blocks$block == 3  | data_alt_blocks$block == 5  | data_alt_blocks$block == 7]  <- "forward"
data_alt_blocks$direction[data_alt_blocks$block == 2 | data_alt_blocks$block == 4  | data_alt_blocks$block == 6  | data_alt_blocks$block == 8]  <- "backward"

# summarize alteration rates per block
data_alt_ID_direction <- data_alt_blocks %>%
  group_by(direction, ID) %>%
  dplyr::summarize(alterations = sum(full_alterations) / 4) # 4 blocks per walking direction

# check normality of distributions 
shapiro.test(data_alt_ID_direction$alterations[data_alt_ID_direction$direction == "forward"])
shapiro.test(data_alt_ID_direction$alterations[data_alt_ID_direction$direction == "backward"])

# test differences in alternation rates
t.test(data_alt_ID_direction$alterations[data_alt_ID_direction$direction == "forward"], data_alt_ID_direction$alterations[data_alt_ID_direction$direction == "backward"], paired = T)

# show summary statistics
describe(data_alt_ID_direction$alterations[data_alt_ID_direction$direction == "forward"])
describe(data_alt_ID_direction$alterations[data_alt_ID_direction$direction == "backward"])


### Mixed Percepts

# extract dataframe with mixed percepts durations per walking conditions
d_non <- data_d %>%
  group_by(ID, walking, p_opticflow) %>%
  dplyr:::summarize(duration_s = sum(duration))
d_non <- d_non[d_non$p_opticflow == "non",]

# get a total duration of all response classes
df <- data_d %>%
  group_by(ID, walking) %>%
  dplyr:::summarize(total_time = sum(duration)) 
# note that total time do not sum exactly to 8*120s since small time differences between consecutive button presses (rows) were not assigned to any of response durations (these gaps equal ~0.025s given 40 Hz sampling rate).

# calculate mixed percepts rate in % (duration (in s)/total time (in s))
d_non <- merge(d_non, df, by = c("ID", "walking"))
d_non$duration <- d_non$duration_s / d_non$total_time

# check normality of distributions
shapiro.test(d_non$duration[d_non$walking == "forward"])
shapiro.test(d_non$duration[d_non$walking == "backward"])

# test differences in mixed percepts
t.test(d_non$duration[d_non$walking == "forward"], d_non$duration[d_non$walking == "backward"], paired = T)

# show summary statistics
mean(d_non$duration[d_non$walking == "forward"])
sd(d_non$duration[d_non$walking == "forward"])
mean(d_non$duration[d_non$walking == "backward"])
sd(d_non$duration[d_non$walking == "backward"])

# overall level of mixed percepts
dt_non <- data_d %>%
  group_by(ID, p_opticflow) %>%
  dplyr:::summarize(duration_s = sum(duration))
dt_non <- dt_non[dt_non$p_opticflow == "non",]

# get a total duration of all response classes
df_all <- data_d %>%
  group_by(ID) %>%
  dplyr:::summarize(total_time = sum(duration)) 

# calculate mixed percepts rate in % (duration (in s)/total time (in s))
dt_non <- merge(dt_non, df_all, by = c("ID"))
dt_non$duration <- dt_non$duration_s / dt_non$total_time

mean(dt_non$duration)
sd(dt_non$duration)

#write.table(dt_non, "SMPVR_data_mixed_percepts", sep = ",")

```


<a name="S5"></a>
&nbsp;

#####**5. Test differences in perceived optic flows between walking conditions ** 

```{r}

# recode percepts as congruent, incongruent, and mixed percepts
data_d$congruency[data_d$walking == "forward" & data_d$p_opticflow == "forward"] <- "congruent"
data_d$congruency[data_d$walking == "forward" & data_d$p_opticflow == "backward"] <- "incongruent"  
data_d$congruency[data_d$walking == "backward" & data_d$p_opticflow == "forward"] <- "incongruent"
data_d$congruency[data_d$walking == "backward" & data_d$p_opticflow == "backward"] <- "congruent"   
data_d$congruency[data_d$p_opticflow == "non"] <- "non"

# get total durations of percepts per each walking condition
df_total_times <- data_d %>%
  group_by(ID, walking) %>%
  dplyr:::summarize(total_time = sum(duration))

# remove mixed percepts
data_d_percepts <- data_d[data_d$p_opticflow != "non",]

# derive durations of optic flow percepts for each subject and condition
d <- data_d_percepts %>%
  group_by(ID, walking, congruency) %>%
  dplyr:::summarize(duration_s = sum(duration))

# merge dataframes with response durations and total times
d <- merge(df_total_times, d, by = c("ID", "walking"))

# calculate proportion of percepts in % (duration (in s)/total time (in s))
d$duration <- (d$duration_s / d$total_time) * 100

# save the data
#write.table(d, "SMPVR_processed_data_main_anova", sep = ",")

# check normality of distributions
shapiro.test(d$duration[d$walking == "forward" & d$congruency == "congruent"])
shapiro.test(d$duration[d$walking == "forward" & d$congruency == "incongruent"])
shapiro.test(d$duration[d$walking == "backward" & d$congruency == "congruent"])
shapiro.test(d$duration[d$walking == "backward" & d$congruency == "incongruent"])

## test the main hypothesis with Repeated-Measures ANOVA
main_anova <- aov_ez("ID","duration", d, within=c("congruency", "walking"), return = afex_options("return_aov"), anova_table = list())

# show summary (in two output versions)
main_anova 
summary(main_anova)

# calculate confidence intervals
conf_intervals <- emmeans::emmeans(main_anova, specs = c("walking", "congruency"))
conf_intervals 

# post-hoc comparisons (Bonferroni method)
emmeans::contrast(conf_intervals ,method="pairwise",adjust="bonferroni")

## show means and standard deviations
# walking forward
mean(d$duration[d$walking == "forward" & d$congruency== "congruent"])
sd(d$duration[d$walking == "forward" & d$congruency== "congruent"])
mean(d$duration[d$walking == "forward" & d$congruency== "incongruent"])
sd(d$duration[d$walking == "forward" & d$congruency== "incongruent"])
# walking backward
mean(d$duration[d$walking == "backward" & d$congruency== "incongruent"])
sd(d$duration[d$walking == "backward" & d$congruency== "incongruent"])
mean(d$duration[d$walking == "backward" & d$congruency== "congruent"])
sd(d$duration[d$walking == "backward" & d$congruency== "congruent"])

# calculate the upper and lower limits of the effect size (used to calculate the sample size for Study 2)
Lims <- conf.limits.ncf(F.value = 52.1, conf.level = 0.90, df.1 <- 1, df.2 <- 28) 
Lower.lim <- Lims$Lower.Limit/(Lims$Lower.Limit + df.1 + df.2 + 1)
Upper.lim <- Lims$Upper.Limit/(Lims$Upper.Limit + df.1 + df.2 + 1)
Lower.lim
Upper.lim
#Lower.lim estimate serves as an input for: https://www.psychometrica.de/effect_size.html (for more details see the relevant paragraph in the main manuscript)

```


<a name="S6"></a>
&nbsp;

#####**6. Plot differences in perceived optic flows between walking conditions** 

```{r}

## Helpful resources used to create the current plots
# https://psyteachr.github.io/msc-conv-f2f/vis.html
# https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R

# define functions used in plotting
"%||%" <- function(a, b) {
  if (!is.null(a)) a else b
}

geom_flat_violin <- function(mapping = NULL, data = NULL, stat = "ydensity",
                             position = "dodge", trim = TRUE, scale = "area",
                             show.legend = NA, inherit.aes = TRUE, ...) {
  layer(
    data = data,
    mapping = mapping,
    stat = stat,
    geom = GeomFlatViolin,
    position = position,
    show.legend = show.legend,
    inherit.aes = inherit.aes,
    params = list(
      trim = trim,
      scale = scale,
      ...
    )
  )
}

GeomFlatViolin <-
  ggproto("Violinist", Geom,
          setup_data = function(data, params) {
            data$width <- data$width %||%
              params$width %||% (resolution(data$x, FALSE) * 0.6) # 0.7
            
            # ymin, ymax, xmin, and xmax define the bounding rectangle for each group
            data %>%
              group_by(group) %>%
              mutate(ymin = min(y),
                     ymax = max(y),
                     xmin = x,
                     xmax = x + width / 2)
            
          },
          
          draw_group = function(data, panel_scales, coord) {
            # Find the points for the line to go all the way around
            data <- transform(data, xminv = x,
                              xmaxv = x + violinwidth * (xmax - x))
            
            # Make sure it's sorted properly to draw the outline
            newdata <- rbind(plyr::arrange(transform(data, x = xminv), y),
                             plyr::arrange(transform(data, x = xmaxv), -y))
            
            # Close the polygon: set first and last point the same
            # Needed for coord_polar and such
            newdata <- rbind(newdata, newdata[1,])
            
            ggplot2:::ggname("geom_flat_violin", GeomPolygon$draw_panel(newdata, panel_scales, coord))
          },
          
          draw_key = draw_key_polygon,
          
          default_aes = aes(weight = 1, colour = "grey20", fill = "white", size = 0.4, #thickness of line
                            alpha = NA, linetype = "solid"), # solid
          
          required_aes = c("x", "y"))

# reorder factor levels
d$walking <- ordered(d$walking, levels = c("forward", "backward"))
d$congruency<- ordered(d$congruency, levels = c("incongruent", "congruent"))

# derive data for raincloud plots
summary_data <-d %>%
  group_by(walking, congruency) %>% 
  dplyr::summarise(mean = mean(duration, na.rm = TRUE),
            min = mean(duration) - qnorm(0.975)*sd(duration)/sqrt(n()),
            max = mean(duration) + qnorm(0.975)*sd(duration)/sqrt(n()),
            sd = sd(duration))
  
# create the plot (exported size: 4 x 5.5)
main_plot <- ggplot(d, aes(x = walking, y = duration, fill = congruency))+
  geom_flat_violin(position = position_nudge(x = .15, y = 0), 
                   trim=FALSE, alpha = 0.6) +
  geom_point((aes(colour = factor(congruency))), position = position_jitter(width = .05, height = 0.05), 
             size = 1.2, alpha = .5, show.legend = FALSE)+
  geom_boxplot(width = .17, outlier.shape = NA, alpha = 0.6, position = "dodge", size = 0.3)+
  geom_pointrange(
    data = summary_data,
    aes(walking, mean, ymin=min, ymax=max),
    shape = 19, size = 0.5, alpha = 0.6,
    position = position_dodge(width = 0.16),
    show.legend = FALSE) +
  scale_fill_manual(values=c(darken("dodgerblue4", amount = 0.7), darken("#FDE725FF", amount = 0.05))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.title = element_text(face="bold",size=14, colour = "black"), axis.text = element_text(face="bold",size=14, colour = "black")) + scale_color_manual(values=c(darken("dodgerblue4", amount = 0.7), darken("#FDE725FF", amount = 0.15))) # 

main_plot

## create alternative version with individual points aligned next to the boxplots
# ggplot(d, aes(x = walking, y = duration, fill = congruency))+
#   geom_flat_violin(position = position_nudge(x = .25, y = 0), 
#                    trim=FALSE, alpha = 0.6) +
#   geom_point((aes(colour = factor(congruency))), position = position_nudge(x = .15, y = 0), 
#              size = 1, alpha = .5, show.legend = FALSE)+
#   geom_boxplot(width = .17, outlier.shape = NA, alpha = 0.6, position = "dodge", size = 0.3)+
#   geom_pointrange(
#     data = summary_data,
#     aes(walking, mean, ymin=min, ymax=max),
#     shape = 19, size = 0.5, alpha = 0.6,
#     position = position_dodge(width = 0.16),
#     show.legend = FALSE) +
#   scale_fill_manual(values=c(darken("dodgerblue4", amount = 0.7), darken("#FDE725FF", amount = 0.05))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_blank(), axis.line = element_line(colour = "black")) + scale_color_manual(values=c(darken("dodgerblue4", amount = 0.7), darken("#FDE725FF", amount = 0.05)))

```

<a name="S7"></a>
&nbsp;

#####**7. Test and plot differences in perceived optic flows between walking conditions - the unfiltered data (before exclusions) ** 

```{r}

# recode percepts as congruent, incongruent, or nons (mixed percepts)
data_d_unfilt$congruency[data_d_unfilt$walking == "forward" & data_d_unfilt$p_opticflow == "forward"] <- "congruent"
data_d_unfilt$congruency[data_d_unfilt$walking == "forward" & data_d_unfilt$p_opticflow == "backward"] <- "incongruent"  
data_d_unfilt$congruency[data_d_unfilt$walking == "backward" & data_d_unfilt$p_opticflow == "forward"] <- "incongruent"
data_d_unfilt$congruency[data_d_unfilt$walking == "backward" & data_d_unfilt$p_opticflow == "backward"] <- "congruent"   
data_d_unfilt$congruency[data_d_unfilt$p_opticflow == "non"] <- "non"

# get total duration of all responses per each walking condition
df_total_times <- data_d_unfilt %>%
  group_by(ID, walking) %>%
  dplyr:::summarize(total_time = sum(duration))

# remove mixed percepts
data_d_unfilt_percepts <- data_d_unfilt[data_d_unfilt$p_opticflow != "non",]

# derive durations of responses for each subject and condition
d_unfilt <- data_d_unfilt_percepts %>%
  group_by(ID, walking, congruency) %>%
  dplyr:::summarize(duration_s = sum(duration))

# merge dataframes with response durations and total times
d_unfilt <- merge(df_total_times, d_unfilt, by = c("ID", "walking"))

# calculate proportion of percepts in % (duration (in s)/total time (in s))
d_unfilt$duration <- (d_unfilt$duration_s / d_unfilt$total_time) * 100

## test the main hypothesis with Repeated-Measures ANOVA
main_anova <- aov_ez("ID","duration", d_unfilt, within=c("congruency", "walking"), return = afex_options("return_aov"), anova_table = list())

# show summary (in two output versions)
main_anova 
summary(main_anova)

# calculate confidence intervals
conf_intervals <- emmeans::emmeans(main_anova, specs = c("walking", "congruency"))
conf_intervals 

# post-hoc comparisons (Bonferroni method)
emmeans::contrast(conf_intervals ,method="pairwise",adjust="bonferroni")

## show means and standard deviations
# walking forward
mean(d_unfilt$duration[d_unfilt$walking == "forward" & d_unfilt$congruency== "congruent"])
sd(d_unfilt$duration[d_unfilt$walking == "forward" & d_unfilt$congruency== "congruent"])
mean(d_unfilt$duration[d_unfilt$walking == "forward" & d_unfilt$congruency== "incongruent"])
sd(d_unfilt$duration[d_unfilt$walking == "forward" & d_unfilt$congruency== "incongruent"])
# walking backward
mean(d_unfilt$duration[d_unfilt$walking == "backward" & d_unfilt$congruency== "incongruent"])
sd(d_unfilt$duration[d_unfilt$walking == "backward" & d_unfilt$congruency== "incongruent"])
mean(d_unfilt$duration[d_unfilt$walking == "backward" & d_unfilt$congruency== "congruent"])
sd(d_unfilt$duration[d_unfilt$walking == "backward" & d_unfilt$congruency== "congruent"])

# reorder factor levels
d_unfilt$walking <- ordered(d_unfilt$walking, levels = c("forward", "backward"))
d_unfilt$congruency<- ordered(d_unfilt$congruency, levels = c("incongruent", "congruent"))

# derive data for raincloud plots
summary_data_unfilt <- d_unfilt %>%
  group_by(walking, congruency) %>% 
  dplyr::summarise(mean = mean(duration, na.rm = TRUE),
            min = mean(duration) - qnorm(0.975)*sd(duration)/sqrt(n()),
            max = mean(duration) + qnorm(0.975)*sd(duration)/sqrt(n()),
            sd = sd(duration))
  
# create the plot (exporting size: 5 x 7 cm)
main_plot_unfiltered <- ggplot(d_unfilt, aes(x = walking, y = duration, fill = congruency))+
  geom_flat_violin(position = position_nudge(x = .15, y = 0), trim=FALSE, alpha = 0.6) +
  geom_point((aes(colour = factor(congruency))), position = position_jitter(width = .05, height = 0.05), size = 1.1, alpha = .5, show.legend = FALSE)+
  geom_boxplot(width = .17, outlier.shape = NA, alpha = 0.6, position = "dodge", size = 0.3)+
  geom_pointrange(
    data = summary_data_unfilt,
    aes(walking, mean, ymin=min, ymax=max),
    shape = 19, size = 0.5, alpha = 0.6,
    position = position_dodge(width = 0.16),
    show.legend = FALSE) +
  scale_fill_manual(values=c(darken("dodgerblue4", amount = 0.7), darken("#FDE725FF", amount = 0.05))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.text = element_text(face="bold",size=14, colour = "black"), axis.title = element_text(face="bold",size=14, colour = "black")) + scale_color_manual(values=c(darken("dodgerblue4", amount = 0.7), darken("#FDE725FF", amount = 0.15))) + scale_y_continuous(breaks = c(0,20,40,60),limits= c(-10, 75))

main_plot_unfiltered

```

<a name="S8"></a>
&nbsp;

#####**8. Relation between proprioceptive precision and the predominance of congruent optic flows ** 

```{r}

### Separate results for walking forward and backward

# reduce proprioception data
data_proprioception <- data_proprioception[,c(1:4)]
data_proprioception <- plyr::rename(data_proprioception, replace = c("diff_M_abs" = "errors")) #the inverse of proprioceptive accuracy
data_proprioception <- plyr::rename(data_proprioception, replace = c("diff_SD" = "variance")) #the inverse of proprioceptive precision

## Walking Forward
dw  <- d[d$walking == "forward",]
dtf <- dw %>%
  group_by(ID, congruency) %>%
  dplyr:::summarize(duration = mean(duration))
dtf <- spread(dtf, congruency, duration)

# derive relative predominance of congruent over incongruent percepts (in %)
dtf$diff_percepts <- dtf$congruent - dtf$incongruent

# merge dataframes
dtf <- merge(dtf, data_proprioception, by = "ID")

# check normality of distributions
shapiro.test(dtf$diff_percepts)
shapiro.test(dtf$variance)

# run Pearson (r) correlation 
cor.test(dtf$diff_percepts, dtf$variance, method = "pearson", alternative = "two.sided", conf.level = 0.95)


## Walking Backward
dw  <- d[d$walking == "backward",]
dtb <- dw %>%
  group_by(ID, congruency) %>%
  dplyr:::summarize(duration = mean(duration))
dtb <- spread(dtb, congruency, duration)

# derive relative predominance of congruent over incongruent percepts (in %)
dtb$diff_percepts <- dtb$congruent - dtb$incongruent

# merge dataframes
dtb <- merge(dtb, data_proprioception, by = "ID")

# check normality of distributions
shapiro.test(dtb$diff_percepts)
shapiro.test(dtb$variance)

# run Spearman correlation
cor.test(dtb$diff_percepts, dtb$variance, method = "spearman", alternative = "two.sided", conf.level = 0.95)

dtff <- dtf[,c(1,4,7)]
dtff$walking <- "forward"

dtbb <- dtb[,c(1,4,7)]
dtbb$walking <- "backward"

dtp <- rbind(dtbb, dtff)

## visualize the relation between variables
proprio_cor_plot <- ggplot(data = dtp, aes(x = variance, y = diff_percepts, colour = walking)) +  geom_smooth(method = "lm", level=0.95, alpha = 0.3, aes(fill=walking)) + geom_point(alpha = 0.6, size = 3, shape = 20) + 
  labs(y = "Predominance of congruent optic flow %", x = "Variance of proprioceptive errors") + scale_color_manual(values = c('forward' = darken("#FDE725FF", amount = 0.27), 'backward' = darken("dodgerblue4", amount = 0.6))) + scale_fill_manual(values = c('forward' = darken("#FDE725FF", amount = 0.27), 'backward' = darken("dodgerblue4", amount = 0.65))) + theme(panel.grid.minor = element_blank(),panel.grid.major = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.text = element_text(face="bold",size=13, colour = "black"), axis.title = element_text(face="bold",size=13, colour = "black"))


# exported manually: size 4 x 5.5
proprio_cor_plot


## equivalent analysis for the unfiltered sample
# modify the data frame
dt_u <- d_unfilt %>%
  group_by(ID, congruency) %>%
  dplyr:::summarize(duration = mean(duration))
dt_u <- spread(dt_u, congruency, duration)

# derive relative predominance of congruent over incongruent percepts (in %)
dt_u$diff_percepts <- dt_u$congruent - dt_u$incongruent

# merge data frames
dt_u <- merge(dt_u, data_proprioception, by = "ID")

# check normality of distributions
shapiro.test(dt_u$diff_percepts)
shapiro.test(dt_u$variance)

# run Pearson (r) correlation 
cor.test(dt_u$diff_percepts, dt_u$variance, method = "spearman", alternative = "two.sided", conf.level = 0.95)
#scatter.smooth(dt_u$variance, dt_u$diff_percepts)


## Overall results

# modify the data frame
dt <- d %>%
  group_by(ID, congruency) %>%
  dplyr:::summarize(duration = mean(duration))
dt <- spread(dt, congruency, duration)

# derive relative predominance of congruent over incongruent percepts (in %)
dt$diff_percepts <- dt$congruent - dt$incongruent

# merge data frames
dt <- merge(dt, data_proprioception, by = "ID")

# check normality of distributions
shapiro.test(dt$diff_percepts)
shapiro.test(dt$variance)

# run Pearson (r) correlation 
cor.test(dt$diff_percepts, dt$variance, method = "pearson", alternative = "two.sided", conf.level = 0.95)

# check normality of distributions (errors instead of variance of errors )
shapiro.test(dt$errors)

# run Pearson (r) correlation (errors instead of variance of errors)
cor.test(dt$diff_percepts, dt$errors, method = "pearson", alternative = "two.sided", conf.level = 0.95)

```

<a name="S9"></a>
&nbsp;

#####**9. Self-reported levels of simulator sickness and sense of presence ** 

```{r}

# participants data (before exclusions)
dq <- data_questionnaires
length(dq$sex[dq$sex == "M"])
length(dq$sex[dq$sex == "W"])
mean(dq$age)
sd(dq$age)
range(dq$age)

# merge data frames
dt <- merge(dt, data_questionnaires, by = "ID")
# participants data (after exclusions)
length(dt$sex[dt$sex == "M"])
length(dt$sex[dt$sex == "W"])
mean(dt$age)
sd(dt$age)
range(dt$age)

# Response confidence (self-reported degree of overlapping between button presses and dominant percepts)
mean(dt$Resp_confidence)
sd(dt$Resp_confidence)
range(dt$Resp_confidence)

# Simulator Sickness Questionnaire
mean(dt$SSQ_pre)
sd(dt$SSQ_pre)
range(dt$SSQ_pre)

mean(dt$SSQ_post)
sd(dt$SSQ_post)
range(dt$SSQ_post)

# difference in cybersickness before/after
shapiro.test(dt$SSQ_pre)
shapiro.test(dt$SSQ_post)
wilcox.test(dt$SSQ_pre, dt$SSQ_post, paired = T, exact = F)
effsize::cohen.d(dt$SSQ_pre, dt$SSQ_post, paired = T)

# VR Presence
mean(dt$VR_presence)
sd(dt$VR_presence)
range(dt$VR_presence)

# explore intercorrelations between items in VR presence questionnaire
vr <- dt[,25:30]
names(vr) <- c("Q1","Q2","Q3", "Q4", "Q5", "Q6")
cor(vr)

# get p values for correlation matrix (source: http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram)
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(vr)

# create matrix
m <- cor(vr)

# plot correlations # (alternative) corrplot(m, type="upper", sig.level = 0.05, method = "circle", tl.col = "black",  p.mat = p.mat)
# export size 7 x 7
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(m, method="color", col=col(200),  
         type="upper",
         addCoef.col = "black", # Add coefficient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         p.mat = p.mat, sig.level = 0.0, insig = "n", number.cex = 1.2, tl.cex= 1.3, cl.cex = 1.2, diag=FALSE)

# show p values
p.mat

### Predominance of congruent percepts and VR presence
shapiro.test(dt$diff_percepts)
shapiro.test(dt$VR_presence)
cor.test(dt$diff_percepts, dt$VR_presence, method = "pearson")

# plot
bold.text <- element_text(face = "bold", color = "black")
presence_plot_pred<- ggplot(data = dt, aes(x = diff_percepts, y = VR_presence)) + geom_smooth(col = "grey8", method = "lm", level=0.95, alpha = 0.5) + 
  geom_point(col = "grey 15", alpha = 0.55, size = 4, shape = 18) + 
  labs(y = "Presence in VR", x = "Predominance of congruent percepts (%)") +  
  scale_fill_manual() + 
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 19), axis.text=element_text(size=16),  axis.title=element_text(size=16), axis.title.y = element_text(margin = margin(t = 0, r = 7, b = 0, l = 0)), axis.title.x =   element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) + 
  scale_y_continuous(limits= c(0, 6),  expand = c(0.01,0.01)) + # breaks = c(2,3,4,5,6,7,8,9,10),
  scale_x_continuous(limits= c(-10, 13),  expand = c(0.01,0.01)) + # breaks = c(2,3,4,5,6,7,8,9,10),
  theme(axis.title = bold.text)

presence_plot_pred # (export 4 x 5)

### Alterations and VR presence
data_alterations_ID <- data_alt_blocks %>%
  group_by(ID) %>%
  dplyr::summarize(alterations = mean(full_alterations))

dt <- merge(dt, data_alterations_ID, by = "ID")

shapiro.test(dt$alterations)
cor.test(dt$alterations, dt$VR_presence, method = "pearson")
#scatter.smooth(dt$alterations, dt$VR_presence, ylab = "Alterations", xlab = "Presence", col = "darkred", cex.lab = 1.4)

### Mixed Percepts and VR Presence
# extract data frame with mixed percepts durations
d_non <- data_d %>%
  group_by(ID, p_opticflow) %>%
  dplyr:::summarize(duration_s = sum(duration))
d_non <- d_non[d_non$p_opticflow == "non",]

# get a total duration of all response classes
d_total <- data_d %>%
  group_by(ID) %>%
  dplyr:::summarize(total_time = sum(duration)) 

# calculate mixed percepts rate in % (duration (in s)/total time (in s))
d_non <- merge(d_non, d_total, by = c("ID"))
d_non$duration_non <- d_non$duration_s / d_non$total_time
d_non <- d_non[,c(1,5)]
d_non$duration_non <- d_non$duration_non * 100
# merge data frames
dt <- merge(dt, d_non, by = "ID")

shapiro.test(dt$duration_non)
cor.test(dt$duration_non, dt$VR_presence, method = "pearson")

bold.text <- element_text(face = "bold", color = "black")
presence_plot <- ggplot(data = dt, aes(x = duration_non, y = VR_presence)) + 
  geom_smooth(col = "grey8", method = "lm", level=0.95, alpha = 0.5) + 
  geom_point(col = "grey 15", alpha = 0.55, size = 4, shape = 18) + 
  labs(y = "Presence in VR", x = "Mixed percepts (%)") +   
  scale_fill_manual() + 
  theme_classic() + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 19), axis.text=element_text(size=16),  axis.title=element_text(size=16), axis.title.y = element_text(margin = margin(t = 0, r = 7, b = 0, l = 0)), axis.title.x =   element_text(margin = margin(t = 10, r = 0, b = 0, l = 0))) + 
  scale_y_continuous(limits= c(0, 6),  expand = c(0.01,0.01)) + # breaks = c(2,3,4,5,6,7,8,9,10),
  scale_x_continuous(limits= c(0, 80),  expand = c(0.01,0.01)) + # breaks = c(2,3,4,5,6,7,8,9,10),
  theme(axis.title = bold.text)

presence_plot # export (4 x 5)

## intercorrelations between mixed percepts, predominance of congruent percepts, and alterations
shapiro.test(dt$duration_non)
shapiro.test(dt$alterations)
shapiro.test(dt$diff_percepts)
cor.test(dt$duration_non, dt$alterations, method = "pearson")
cor.test(dt$alterations, dt$diff_percepts, method = "pearson")
cor.test(dt$duration_non, dt$diff_percepts, method = "pearson")
#scatter.smooth(dt$duration_non, dt$diff_percepts)

## Relation between cybersickness and visual experience
dt$SSQ_diff <- dt$SSQ_post - dt$SSQ_pre
cor.test(dt$SSQ_diff, dt$diff_percepts)
#scatter.smooth(dt$SSQ_diff, dt$diff_percepts)
cor.test(dt$SSQ_diff, dt$duration_non)
#scatter.smooth(dt$SSQ_diff, dt$diff_non)
cor.test(dt$SSQ_diff, dt$alterations)
#scatter.smooth(dt$SSQ_diff, dt$diff_non)

## Relation between cyberickness and presence
shapiro.test(dt$SSQ_diff)
shapiro.test(dt$VR_presence)
cor.test(dt$SSQ_diff, dt$VR_presence)

```



